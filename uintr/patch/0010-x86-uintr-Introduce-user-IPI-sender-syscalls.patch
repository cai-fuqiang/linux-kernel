From e2c660ae020ad33569a40732cb4f4ce07341abf1 Mon Sep 17 00:00:00 2001
From: Sohil Mehta <sohil.mehta@intel.com>
Date: Thu, 8 Sep 2022 15:20:34 -0700
Subject: [PATCH 10/18] x86/uintr: Introduce user IPI sender syscalls

Add a registration syscall for a task to register itself as a user
interrupt sender using the uvec_fd generated by the receiver. A task
can register multiple uvec_fds. Each unique successful connection
creates a new entry in the User Interrupt Target Table (UITT).

Each entry in the UITT table is referred by the UITT index (uipi_index).
The uipi_index returned during the registration syscall lets a sender
generate a user IPI using the 'SENDUIPI <uipi_index>' instruction.

Also, add a sender unregister syscall to unregister a particular task
from the uvec_fd. Calling close on the uvec_fd will disconnect all
threads in a sender process from that FD.

Currently, the UITT size is arbitrarily chosen as 256 entries
corresponding to a 4KB page. Based on feedback and usage data this can
either be increased/decreased or made dynamic later.

Architecturally, the UITT table can be unique for each thread or shared
across threads of the same thread group. The benefit of sharing the UITT
table is that all threads would see the same view of the UITT table.
Also the kernel UITT memory allocation would be more efficient if
multiple threads connect to the same uvec_fd. However, this would mean
the kernel needs to keep the UITT table size MISC_MSR[] in sync across
these threads. Also the UPID/UITT teardown flows might need additional
consideration.

Another option is to keep the UITT as unique for the each thread. This
makes the kernel implemantion relatively simple and only threads that
use uintr get setup with the related structures. However, this means
that the uipi_index for each thread would be inconsistent wrt to other
threads.  (Executing 'SENDUIPI 2' on threads of the same process could
generate different user interrupts.)

The current implementation shares the UITT for tasks of the same process
address space.  A #UD is generated when a task tries to use the UITT
without it being programmed by the kernel. Use that #UD to fix and patch
the task MSRs.

Signed-off-by: Sohil Mehta <sohil.mehta@intel.com>
---
 arch/x86/include/asm/mmu.h         |   4 +
 arch/x86/include/asm/mmu_context.h |  21 +
 arch/x86/include/asm/processor.h   |   1 +
 arch/x86/include/asm/uintr.h       |  39 ++
 arch/x86/include/uapi/asm/uintr.h  |   6 +
 arch/x86/kernel/process.c          |   6 +
 arch/x86/kernel/traps.c            | 104 +++++
 arch/x86/kernel/uintr.c            | 599 ++++++++++++++++++++++++++++-
 8 files changed, 779 insertions(+), 1 deletion(-)

diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h
index 5d7494631ea9..dbf1675c8f5a 100644
--- a/arch/x86/include/asm/mmu.h
+++ b/arch/x86/include/asm/mmu.h
@@ -55,6 +55,10 @@ typedef struct {
 	u16 pkey_allocation_map;
 	s16 execute_only_pkey;
 #endif
+#ifdef CONFIG_X86_USER_INTERRUPTS
+	/* Check: If uitt_ctx needs a better home? */
+	struct uintr_uitt_ctx	*uitt_ctx;
+#endif
 } mm_context_t;
 
 #define INIT_MM_CONTEXT(mm)						\
diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
index b8d40ddeab00..655fbf56e3fd 100644
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@ -12,6 +12,7 @@
 #include <asm/tlbflush.h>
 #include <asm/paravirt.h>
 #include <asm/debugreg.h>
+#include <asm/uintr.h>
 
 extern atomic64_t last_mm_ctx_id;
 
@@ -123,6 +124,9 @@ static inline int init_new_context(struct task_struct *tsk,
 static inline void destroy_context(struct mm_struct *mm)
 {
 	destroy_context_ldt(mm);
+
+	if (cpu_feature_enabled(X86_FEATURE_UINTR))
+		uintr_destroy_uitt_ctx(mm);
 }
 
 extern void switch_mm(struct mm_struct *prev, struct mm_struct *next,
@@ -164,10 +168,27 @@ static inline void arch_dup_pkeys(struct mm_struct *oldmm,
 #endif
 }
 
+static inline void uitt_dup_context(struct mm_struct *oldmm,
+				    struct mm_struct *mm)
+{
+/* TODO: Remove this ugly ifdef */
+#ifdef CONFIG_X86_USER_INTERRUPTS
+
+	if (!cpu_feature_enabled(X86_FEATURE_UINTR))
+		return;
+
+	/* Inherit UINTR state. New additions to UITT will be accessible by both processes */
+	/* Check if this needs a lock */
+	if (oldmm->context.uitt_ctx)
+		mm->context.uitt_ctx = get_uitt_ref(oldmm->context.uitt_ctx);
+#endif
+}
+
 static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
 {
 	arch_dup_pkeys(oldmm, mm);
 	paravirt_arch_dup_mmap(oldmm, mm);
+	uitt_dup_context(oldmm, mm);
 	return ldt_dup_context(oldmm, mm);
 }
 
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 3c227d544f37..0859583f7192 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -526,6 +526,7 @@ struct thread_struct {
 	/* User Interrupt state*/
 
 	/* Signifies whether the MSRs for that thread are active */
+	unsigned int		uitt_activated:1;
 	unsigned int		upid_activated:1;
 
 	/* Pointer to the UPID context for the task */
diff --git a/arch/x86/include/asm/uintr.h b/arch/x86/include/asm/uintr.h
index 636e2f4e7ef4..9726e72ac479 100644
--- a/arch/x86/include/asm/uintr.h
+++ b/arch/x86/include/asm/uintr.h
@@ -28,8 +28,45 @@ struct uintr_upid_ctx {
 	struct uintr_upid *upid;
 	/* TODO: Change to kernel kref api */
 	refcount_t refs;
+	bool receiver_active;		/* Flag for UPID being mapped to a receiver */
 };
 
+/*
+ * Each UITT entry is 16 bytes in size.
+ * Current UITT table size is set as 4KB (256 * 16 bytes)
+ */
+#define UINTR_MAX_UITT_NR 256
+
+/* User Interrupt Target Table Entry (UITTE) */
+struct uintr_uitt_entry {
+	u8	valid;			/* bit 0: valid, bit 1-7: reserved */
+	u8	user_vec;
+	u8	reserved[6];
+	u64	target_upid_addr;
+} __packed __aligned(16);
+
+/* TODO: Remove uitt from struct names */
+struct uintr_uitt_ctx {
+	struct uintr_uitt_entry *uitt;
+	/* Protect UITT */
+	struct mutex uitt_lock;
+	/* TODO: Change to kernel kref api */
+	refcount_t refs;
+	/* track active uitt entries per bit */
+	u64 uitt_mask[BITS_TO_U64(UINTR_MAX_UITT_NR)];
+	/* TODO: Might be useful to use xarray over here as the MAX size increases */
+	struct uintr_upid_ctx *r_upid_ctx[UINTR_MAX_UITT_NR];
+};
+
+/* User IPI sender related functions */
+struct uintr_uitt_ctx *get_uitt_ref(struct uintr_uitt_ctx *uitt_ctx);
+void put_uitt_ref(struct uintr_uitt_ctx *uitt_ctx);
+void uintr_destroy_uitt_ctx(struct mm_struct *mm);
+
+bool is_uintr_sender(struct task_struct *t);
+void uintr_set_sender_msrs(struct task_struct *t);
+bool uintr_check_uitte_valid(struct uintr_uitt_ctx *uitt_ctx, unsigned int entry);
+
 /* TODO: Inline the context switch related functions */
 void switch_uintr_prepare(struct task_struct *prev);
 void switch_uintr_return(void);
@@ -38,6 +75,8 @@ void uintr_free(struct task_struct *task);
 
 #else /* !CONFIG_X86_USER_INTERRUPTS */
 
+static inline void uintr_destroy_uitt_ctx(struct mm_struct *mm) {}
+
 static inline void switch_uintr_prepare(struct task_struct *prev) {}
 static inline void switch_uintr_return(void) {}
 
diff --git a/arch/x86/include/uapi/asm/uintr.h b/arch/x86/include/uapi/asm/uintr.h
index 059f225d3057..6b306bf0e6eb 100644
--- a/arch/x86/include/uapi/asm/uintr.h
+++ b/arch/x86/include/uapi/asm/uintr.h
@@ -10,4 +10,10 @@
 #define UINTR_DISABLE		_IO(UINTR_BASE, 1)
 #define UINTR_NOTIFY		_IO(UINTR_BASE, 2)
 
+/* uipi_fd IOCTLs */
+#define UINTR_UIPI_FD_BASE		'u'
+#define UIPI_SET_TARGET_TABLE		_IO(UINTR_UIPI_FD_BASE, 0)
+/* Not supported for now. UITT clearing is an involved process */
+#define UIPI_CLEAR_TARGET_TABLE		_IO(UINTR_UIPI_FD_BASE, 1)
+
 #endif
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 29cff35d58b5..0b31c5ebab6e 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -98,6 +98,12 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	dst->thread.upid_ctx = NULL;
 
 	dst->thread.upid_activated = false;
+
+	/*
+	 * User Interrupt sender state is shared across tasks of the same mm.
+	 * However, the UITT is activated on-demand
+	 */
+	dst->thread.uitt_activated = false;
 #endif
 
 	/* Drop the copied pointer to current's fpstate */
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index d62b2cb85cea..8254275c45df 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -59,6 +59,7 @@
 #include <asm/fpu/xstate.h>
 #include <asm/vm86.h>
 #include <asm/umip.h>
+#include <asm/uintr.h>
 #include <asm/insn.h>
 #include <asm/insn-eval.h>
 #include <asm/vdso.h>
@@ -324,6 +325,102 @@ static noinstr bool handle_bug(struct pt_regs *regs)
 	return handled;
 }
 
+#ifdef CONFIG_X86_USER_INTERRUPTS
+/**
+ * is_senduipi_insn() - Determine if instruction is a senduipi instruction
+ * @insn:	Instruction containing the opcode to inspect
+ *
+ * Returns:
+ *
+ * true if the instruction, determined by the opcode, is a senduipi
+ * instructions as defined in the Intel Software Development manual.
+ * False otherwise.
+ */
+static bool is_senduipi_insn(struct insn *insn)
+{
+	pr_debug("insn->opcode: nbytes %d, byte[0]:%x, byte[1]:%x, byte[2]:%x, byte[3]:%x\n",
+		 insn->opcode.nbytes,
+		 insn->opcode.bytes[0],
+		 insn->opcode.bytes[1],
+		 insn->opcode.bytes[2],
+		 insn->opcode.bytes[3]);
+
+	/* SENDUIPI instruction size is 2 bytes. */
+	if (insn->opcode.nbytes != 2)
+		return false;
+
+	if ((insn->opcode.bytes[0] == 0xf) && (insn->opcode.bytes[1] == 0xc7))
+		return true;
+
+	return false;
+}
+
+/* Check if this needs to run with interrupts disabled */
+/*
+ * No prints in this function to avoid the warning exc_invalid_op()+0x6f: call
+ * to __dynamic_pr_debug() leaves .noinstr.text section
+ */
+static bool fixup_senduipi_ud_exception(struct pt_regs *regs)
+{
+	struct task_struct *t = current;
+	struct uintr_uitt_ctx *uitt_ctx;
+	unsigned char buf[MAX_INSN_SIZE];
+	struct insn insn;
+	//long uipi_index;
+	int nr_copied;
+
+	//pr_debug("uintr: In ud exception fix function\n");
+
+	/* Check if the UITT is already activated */
+	if (is_uintr_sender(t))
+		return false;
+
+	uitt_ctx = t->mm->context.uitt_ctx;
+	if (!uitt_ctx)
+		return false;
+
+	if (!regs)
+		return false;
+
+	/*
+	 * The SENDUIPI instruction decoding could be avoided based on the fact
+	 * that the UITT hasn't been activated but the mm has a uitt_ctx. This
+	 * would be a reasonable guess.
+	 *
+	 * For now, avoid the optimization and do the hard work.
+	 */
+
+	//pr_debug("uintr: Starting to Decode fault instruction\n");
+
+	nr_copied = insn_fetch_from_user(regs, buf);
+	if (nr_copied <= 0)
+		return false;
+
+	if (!insn_decode_from_regs(&insn, regs, buf, nr_copied))
+		return false;
+
+	if (!is_senduipi_insn(&insn))
+		return false;
+
+	//pr_debug("uintr: senduipi instruction detected. Activate the sender MSRs");
+	uintr_set_sender_msrs(t);
+
+	/* SENDUIPI index is not important in this case. Also it wouldn't generate a #UD */
+#if 0
+	uipi_index = senduipi_decode_index(&insn, regs);
+	if (uipi_index < 0)
+		return false;
+
+	pr_debug("uintr: Re-executing SENDUIPI\n");
+#endif
+	return true;
+}
+
+#else
+static inline bool is_senduipi_insn(struct insn *insn) { return false; }
+static inline bool fixup_senduipi_ud_exception(struct pt_regs *regs) { return false; }
+#endif
+
 DEFINE_IDTENTRY_RAW(exc_invalid_op)
 {
 	irqentry_state_t state;
@@ -336,6 +433,13 @@ DEFINE_IDTENTRY_RAW(exc_invalid_op)
 	if (!user_mode(regs) && handle_bug(regs))
 		return;
 
+	/* Check: Where should the senduipi fix be done in this function exactly? */
+	/* Check: Does this need cond_local_irq_enable/disable pair? */
+	if (cpu_feature_enabled(X86_FEATURE_UINTR)) {
+		if (user_mode(regs) && fixup_senduipi_ud_exception(regs))
+			return;
+	}
+
 	state = irqentry_enter(regs);
 	instrumentation_begin();
 	handle_invalid_op(regs);
diff --git a/arch/x86/kernel/uintr.c b/arch/x86/kernel/uintr.c
index 0f38e1061009..1821e011f695 100644
--- a/arch/x86/kernel/uintr.c
+++ b/arch/x86/kernel/uintr.c
@@ -44,6 +44,16 @@ inline bool is_uintr_receiver(struct task_struct *t)
 	return !!t->thread.upid_activated;
 }
 
+inline bool is_uintr_sender(struct task_struct *t)
+{
+	return !!t->thread.uitt_activated;
+}
+
+inline bool is_uintr_task(struct task_struct *t)
+{
+	return(is_uintr_receiver(t) || is_uintr_sender(t));
+}
+
 static void free_upid(struct uintr_upid_ctx *upid_ctx)
 {
 	put_task_struct(upid_ctx->task);
@@ -99,6 +109,7 @@ static struct uintr_upid_ctx *alloc_upid(void)
 	upid_ctx->upid = upid;
 	refcount_set(&upid_ctx->refs, 1);
 	upid_ctx->task = get_task_struct(current);
+	upid_ctx->receiver_active = true;
 
 	return upid_ctx;
 }
@@ -248,6 +259,581 @@ SYSCALL_DEFINE2(uintr_vector_fd, u64, vector, unsigned int, flags)
 	return ret;
 }
 
+static inline bool is_uitt_empty(struct uintr_uitt_ctx *uitt_ctx)
+{
+	return !!bitmap_empty((unsigned long *)uitt_ctx->uitt_mask,
+			      UINTR_MAX_UITT_NR);
+}
+
+static int check_uitt_ref(struct uintr_uitt_ctx *uitt_ctx)
+{
+	return refcount_read(&uitt_ctx->refs);
+}
+
+static void free_uitt_entry(struct uintr_uitt_ctx *uitt_ctx, unsigned int entry)
+{
+	if (WARN_ON_ONCE(entry >= UINTR_MAX_UITT_NR))
+		return;
+
+	pr_debug("send: Freeing UITTE entry %d for uitt_ctx=%lx\n",
+		 entry, (unsigned long)uitt_ctx);
+
+	put_upid_ref(uitt_ctx->r_upid_ctx[entry]);
+
+	mutex_lock(&uitt_ctx->uitt_lock);
+	memset(&uitt_ctx->uitt[entry], 0, sizeof(struct uintr_uitt_entry));
+	mutex_unlock(&uitt_ctx->uitt_lock);
+
+	clear_bit(entry, (unsigned long *)uitt_ctx->uitt_mask);
+
+	if (is_uitt_empty(uitt_ctx)) {
+		pr_debug("send: UITT mask is empty. UITT refcount=%d\n",
+			 check_uitt_ref(uitt_ctx));
+		/*
+		 * Tearing down UITT is not simple. Multiple tasks would have
+		 * their UITT MSR programmed. Instead of doing it right now
+		 * delay the freeing to MM exit.
+		 * TODO: Confirm uitt ref count is accurate.
+		 */
+		//teardown_uitt();
+	}
+}
+
+/* TODO: Fix locking and atomicity of updates */
+static void free_all_uitt_entries(struct uintr_uitt_ctx *uitt_ctx)
+{
+	int entry = find_first_bit((unsigned long *)uitt_ctx->uitt_mask,
+				       UINTR_MAX_UITT_NR);
+
+	while (entry != UINTR_MAX_UITT_NR) {
+		free_uitt_entry(uitt_ctx, entry);
+		entry = find_first_bit((unsigned long *)uitt_ctx->uitt_mask,
+				       UINTR_MAX_UITT_NR);
+	}
+}
+
+static void free_uitt(struct uintr_uitt_ctx *uitt_ctx)
+{
+	if (!is_uitt_empty(uitt_ctx)) {
+		pr_debug("UITT: being freed but mask not empty\n");
+		free_all_uitt_entries(uitt_ctx);
+	}
+
+	mutex_lock(&uitt_ctx->uitt_lock);
+
+	kfree(uitt_ctx->uitt);
+	uitt_ctx->uitt = NULL;
+	mutex_unlock(&uitt_ctx->uitt_lock);
+
+	kfree(uitt_ctx);
+}
+
+void put_uitt_ref(struct uintr_uitt_ctx *uitt_ctx)
+{
+	if (refcount_dec_and_test(&uitt_ctx->refs)) {
+		pr_debug("UITT: %px Decrement refcount=0 Freeing UITT\n",
+			 uitt_ctx->uitt);
+		free_uitt(uitt_ctx);
+	} else {
+		pr_debug("UITT: %px Decrement refcount=%d\n",
+			 uitt_ctx->uitt,
+			 refcount_read(&uitt_ctx->refs));
+	}
+}
+
+/* TODO: Confirm if this function is accurate */
+void uintr_destroy_uitt_ctx(struct mm_struct *mm)
+{
+	if (mm->context.uitt_ctx) {
+		pr_debug("mm exit: uitt ref: %d\n", check_uitt_ref(mm->context.uitt_ctx));
+		put_uitt_ref(mm->context.uitt_ctx);
+		//teardown_uitt(mm->context.uitt_ctx);
+		mm->context.uitt_ctx = NULL;
+	}
+}
+
+/* TODO: Replace UITT allocation with KPTI compatible memory allocator */
+static struct uintr_uitt_ctx *alloc_uitt(void)
+{
+	struct uintr_uitt_ctx *uitt_ctx;
+	struct uintr_uitt_entry *uitt;
+
+	uitt_ctx = kzalloc(sizeof(*uitt_ctx), GFP_KERNEL);
+	if (!uitt_ctx)
+		return NULL;
+
+	uitt = kzalloc(sizeof(*uitt) * UINTR_MAX_UITT_NR, GFP_KERNEL);
+	if (!uitt) {
+		kfree(uitt_ctx);
+		return NULL;
+	}
+
+	uitt_ctx->uitt = uitt;
+	mutex_init(&uitt_ctx->uitt_lock);
+	refcount_set(&uitt_ctx->refs, 1);
+
+	return uitt_ctx;
+}
+
+struct uintr_uitt_ctx *get_uitt_ref(struct uintr_uitt_ctx *uitt_ctx)
+{
+	refcount_inc(&uitt_ctx->refs);
+	pr_debug("UITT: %px Increment refcount=%d\n",
+		 uitt_ctx->uitt, refcount_read(&uitt_ctx->refs));
+
+	return uitt_ctx;
+}
+
+static inline void mark_uitte_invalid(struct uintr_uitt_ctx *uitt_ctx, unsigned int uitt_index)
+{
+	struct uintr_uitt_entry *uitte;
+
+	mutex_lock(&uitt_ctx->uitt_lock);
+	uitte = &uitt_ctx->uitt[uitt_index];
+	uitte->valid = 0;
+	mutex_unlock(&uitt_ctx->uitt_lock);
+}
+
+static int init_uitt_ctx(void)
+{
+	struct mm_struct *mm = current->mm;
+	struct uintr_uitt_ctx *uitt_ctx;
+
+	uitt_ctx = alloc_uitt();
+	if (!uitt_ctx) {
+		pr_debug("send: Alloc UITT failed for task=%d\n", current->pid);
+		return -ENOMEM;
+	}
+
+	pr_debug("send: Setup a new UITT=%px for mm=%lx with size %d\n",
+		 uitt_ctx->uitt, (unsigned long)mm, UINTR_MAX_UITT_NR * 16);
+
+	/* The UITT is allocated with a ref count of 1 */
+	mm->context.uitt_ctx = uitt_ctx;
+
+	return 0;
+}
+
+void uintr_set_sender_msrs(struct task_struct *t)
+{
+	struct uintr_uitt_ctx *uitt_ctx = t->mm->context.uitt_ctx;
+	void *xstate;
+	u64 msr64;
+
+	/* Maybe WARN_ON_FPU */
+	WARN_ON_ONCE(t != current);
+
+	xstate = start_update_xsave_msrs(XFEATURE_UINTR);
+
+	xsave_wrmsrl(xstate, MSR_IA32_UINTR_TT, (u64)uitt_ctx->uitt | 1);
+	xsave_rdmsrl(xstate, MSR_IA32_UINTR_MISC, &msr64);
+	msr64 &= GENMASK_ULL(63, 32);
+	msr64 |= UINTR_MAX_UITT_NR - 1;
+	xsave_wrmsrl(xstate, MSR_IA32_UINTR_MISC, msr64);
+
+	end_update_xsave_msrs();
+
+	t->thread.uitt_activated = true;
+}
+
+bool uintr_check_uitte_valid(struct uintr_uitt_ctx *uitt_ctx, unsigned int entry)
+{
+	return !!test_bit(entry, (unsigned long *)uitt_ctx->uitt_mask);
+}
+
+/* TODO: Fix unregister flow. Also all modifications to the uitte should be under a single lock */
+static int do_uintr_unregister_sender(struct uintr_uitt_ctx *uitt_ctx, unsigned int entry)
+{
+	/* Check if the supplied UITT index is valid */
+	if (!uintr_check_uitte_valid(uitt_ctx, entry)) {
+		pr_debug("send: Unregister for invalid UITTE %d for uitt_ctx=%lx\n",
+			 entry, (unsigned long)uitt_ctx);
+		return -EINVAL;
+	}
+
+	/* To make sure any new senduipi result in a #GP fault. */
+	/*
+	 * Check: Can the UITTE be modified directly. What is some other cpu
+	 * (ucode) is concurrently accessing it?
+	 */
+	mark_uitte_invalid(uitt_ctx, entry);
+
+	pr_debug("send: Freeing UITTE %d for uitt_ctx=%lx\n",
+		 entry, (unsigned long)uitt_ctx);
+
+	/*
+	 * Check: Find a good way to free the uitte entry. Can we free the UITT
+	 * directly instead of marking it invalid above?
+	 */
+	free_uitt_entry(uitt_ctx, entry);
+
+	/* TODO: Verify UPID & UITT reference counting */
+	//put_uitt_ref(uitt_ctx);
+
+	return 0;
+}
+
+static int uintr_init_sender(struct task_struct *t)
+{
+	struct mm_struct *mm = t->mm;
+	int ret = 0;
+
+	/* what about concurrency here? */
+	if (!mm->context.uitt_ctx)
+		ret = init_uitt_ctx();
+
+	return ret;
+}
+
+/*
+ * No lock is needed to read the active flag. Writes only happen from
+ * r_info->task that owns the UPID. Everyone else would just read this flag.
+ *
+ * This only provides a static check. The receiver may become inactive right
+ * after this check. The primary reason to have this check is to prevent future
+ * senders from connecting with this UPID, since the receiver task has already
+ * made this UPID inactive.
+ */
+static bool uintr_is_receiver_active(struct uintr_upid_ctx *upid_ctx)
+{
+	return upid_ctx->receiver_active;
+}
+
+static int do_uintr_register_sender(u64 uvec, struct uintr_upid_ctx *upid_ctx)
+{
+	struct uintr_uitt_entry *uitte = NULL;
+	struct uintr_uitt_ctx *uitt_ctx;
+	struct task_struct *t = current;
+	int entry;
+	int ret;
+
+	/*
+	 * Only a static check. Receiver could exit anytime after this check.
+	 * This check only prevents connections using uvec_fd after the
+	 * receiver has already exited/unregistered.
+	 */
+	if (!uintr_is_receiver_active(upid_ctx))
+		return -ESHUTDOWN;
+
+	ret = uintr_init_sender(t);
+	if (ret)
+		return ret;
+
+	uitt_ctx = t->mm->context.uitt_ctx;
+
+	BUILD_BUG_ON(UINTR_MAX_UITT_NR < 1);
+
+	/* TODO: Need a lock to prevent concurrent access to uitt_mask */
+	entry = find_first_zero_bit((unsigned long *)uitt_ctx->uitt_mask,
+				    UINTR_MAX_UITT_NR);
+	if (entry >= UINTR_MAX_UITT_NR)
+		return -ENOSPC;
+
+	set_bit(entry, (unsigned long *)uitt_ctx->uitt_mask);
+
+	mutex_lock(&uitt_ctx->uitt_lock);
+
+	uitte = &uitt_ctx->uitt[entry];
+	pr_debug("send: sender=%d receiver=%d UITTE entry %d address %px\n",
+		 t->pid, upid_ctx->task->pid, entry, uitte);
+
+	/* Program the UITT entry */
+	uitte->user_vec = uvec;
+	uitte->target_upid_addr = (u64)upid_ctx->upid;
+	uitte->valid = 1;
+
+	uitt_ctx->r_upid_ctx[entry] = get_upid_ref(upid_ctx);
+
+	mutex_unlock(&uitt_ctx->uitt_lock);
+
+	//s_info->uitt_ctx = get_uitt_ref(uitt_ctx);
+	//s_info->task = get_task_struct(current);
+	//s_info->uitt_index = entry;
+
+	if (!is_uintr_sender(t))
+		uintr_set_sender_msrs(t);
+
+	return entry;
+}
+
+static long uipifd_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct uintr_uitt_ctx *uitt_ctx = file->private_data;
+	struct task_struct *t = current;
+	int ret = 0;
+
+	switch (cmd) {
+	case UIPI_SET_TARGET_TABLE:
+
+		pr_debug("send: uipi fd SET TT IOCTL task=%d\n", t->pid);
+
+		/*
+		 * Clearing the UITT is a more involved procedure. The UITT
+		 * could be in use across multiple processes.
+		 */
+		if (t->mm->context.uitt_ctx) {
+			pr_debug("send: uitt_ctx is already set in mm for task=%d UITT=%px\n",
+				 t->pid, t->mm->context.uitt_ctx->uitt);
+			ret = -EBUSY;
+			break;
+		}
+
+		t->mm->context.uitt_ctx = get_uitt_ref(uitt_ctx);
+
+		/*
+		 * Proactively set the sender MSRs for this task. This helps
+		 * avoid the trap and instruction decode
+		 */
+		uintr_set_sender_msrs(t);
+
+		pr_debug("send: uitt_ctx is being set in mm for task=%d UITT=%px\n",
+			 t->pid, uitt_ctx->uitt);
+		break;
+
+#if 0
+	case UIPI_CLEAR_TARGET_TABLE:
+
+		/*
+		 * Clearing the UITT is a more involved procedure. The UITT
+		 * could be in use across multiple processes.
+		 */
+		ret = -EOPNOTSUPP;
+		pr_debug("send: uipi fd CLEAR TT IOCTL is not supported task=%d\n",
+			 t->pid);
+		break;
+#endif
+
+	default:
+		pr_debug("send: Invalid uipi_fd IOCTL command %d\n", cmd);
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static int uipifd_release(struct inode *inode, struct file *file)
+{
+	struct uintr_uitt_ctx *uitt_ctx = file->private_data;
+
+	pr_debug("send: Releasing uipi fd for task=%d UITT=%px\n",
+		 current->pid, uitt_ctx->uitt);
+
+	/* UITT stays allocated until the process dies, no need to clear mm */
+
+	put_uitt_ref(uitt_ctx);
+
+	return 0;
+}
+
+static int uipifd_open(struct inode *inode, struct file *file)
+{
+	struct task_struct *t = current;
+	struct uintr_uitt_ctx *uitt_ctx;
+
+	pr_debug("send: uipi fd opened task=%d\n", current->pid);
+
+	uitt_ctx = t->mm->context.uitt_ctx;
+	/* TODO: Figure out how the ordering works in this case */
+	if (!uitt_ctx) {
+		uitt_ctx = file->private_data;
+		t->mm->context.uitt_ctx = uitt_ctx;
+		pr_debug("send: uitt_ctx is being set in mm for task=%d UITT=%px\n",
+			 current->pid, uitt_ctx->uitt);
+	} else {
+		pr_debug("send: uitt_ctx already set in mm for task=%d UITT=%px\n",
+			 current->pid, uitt_ctx->uitt);
+	}
+
+	return 0;
+}
+
+static const struct file_operations uipifd_fops = {
+#ifdef CONFIG_PROC_FS
+	//.show_fdinfo	= uipifd_show_fdinfo,
+#endif
+	.unlocked_ioctl = uipifd_ioctl,
+	.open		= uipifd_open,
+	.release	= uipifd_release,
+	.llseek		= noop_llseek,
+};
+
+/*
+ * sys_uintr_ipi_fd - Create a uipi_fd to execute SENDUIPI.
+ */
+SYSCALL_DEFINE1(uintr_ipi_fd, unsigned int, flags)
+{
+	struct uintr_uitt_ctx *uitt_ctx;
+	int uipi_fd;
+	int ret;
+
+	if (!cpu_feature_enabled(X86_FEATURE_UINTR))
+		return -ENOSYS;
+
+	if (flags)
+		return -EINVAL;
+
+	ret = uintr_init_sender(current);
+	if (ret)
+		return ret;
+
+	uitt_ctx = get_uitt_ref(current->mm->context.uitt_ctx);
+
+	/* TODO: Get user input for flags - UFD_CLOEXEC? */
+	/* Check: Do we need O_NONBLOCK? */
+	uipi_fd = anon_inode_getfd("[uipi_fd]", &uipifd_fops, uitt_ctx,
+				   O_RDONLY | O_CLOEXEC | O_NONBLOCK);
+
+	if (uipi_fd < 0) {
+		put_uitt_ref(uitt_ctx);
+		pr_debug("send: uipi_fd create failed for task=%d ret %d\n",
+			 current->pid, ret);
+	} else {
+		pr_debug("send: Alloc success uipi_fd %d for task=%d\n",
+			 uipi_fd, current->pid);
+	}
+
+	return uipi_fd;
+}
+
+/*
+ * sys_uintr_register_sender - setup user inter-processor interrupt sender.
+ */
+SYSCALL_DEFINE2(uintr_register_sender, int, uvecfd, unsigned int, flags)
+{
+	//struct uintr_uitt_ctx *uitt_ctx;
+	//struct uintr_sender_info *s_info;
+	struct uvecfd_ctx *uvecfd_ctx;
+	//unsigned long lock_flags;
+	struct file *uvec_f;
+	struct fd f;
+	int ret = 0;
+
+	if (!cpu_feature_enabled(X86_FEATURE_UINTR))
+		return -ENOSYS;
+
+	if (flags)
+		return -EINVAL;
+
+	f = fdget(uvecfd);
+	uvec_f = f.file;
+	if (!uvec_f)
+		return -EBADF;
+
+	if (uvec_f->f_op != &uvecfd_fops) {
+		ret = -EOPNOTSUPP;
+		goto out_fdput;
+	}
+
+	uvecfd_ctx = (struct uvecfd_ctx *)uvec_f->private_data;
+
+	/*
+	 * We would need to store the sender list in order to detect if a
+	 * connection has already been made. Also care must taken to
+	 * incorporate concurrent modifications to the uitt_ctx
+	 */
+#if 0
+	uitt_ctx = current->mm->context.uitt_ctx;
+
+	/* Detect if a connection has already been made */
+	if (uitt_ctx) {
+		spin_lock_irqsave(&uvecfd_ctx->sender_lock, lock_flags);
+		list_for_each_entry(s_info, &uvecfd_ctx->sender_list, node) {
+			if (s_info->uitt_ctx == uitt_ctx) {
+				ret = -EISCONN;
+				break;
+			}
+		}
+		spin_unlock_irqrestore(&uvecfd_ctx->sender_lock, lock_flags);
+
+		if (ret)
+			goto out_fdput;
+	}
+
+	s_info = kzalloc(sizeof(*s_info), GFP_KERNEL);
+	if (!s_info) {
+		ret = -ENOMEM;
+		goto out_fdput;
+	}
+#endif
+
+	ret = do_uintr_register_sender(uvecfd_ctx->uvec, uvecfd_ctx->upid_ctx);
+	if (ret < 0) {
+		//kfree(s_info);
+		goto out_fdput;
+	}
+
+	//spin_lock_irqsave(&uvecfd_ctx->sender_lock, lock_flags);
+	//list_add(&s_info->node, &uvecfd_ctx->sender_list);
+	//spin_unlock_irqrestore(&uvecfd_ctx->sender_lock, lock_flags);
+
+	//ret = s_info->uitt_index;
+
+out_fdput:
+	pr_debug("send: register sender task=%d flags %d ret(uipi_id)=%d\n",
+		 current->pid, flags, ret);
+
+	fdput(f);
+	return ret;
+}
+
+/*
+ * sys_uintr_unregister_sender - Unregister user inter-processor interrupt sender.
+ */
+SYSCALL_DEFINE2(uintr_unregister_sender, int, uipi_index, unsigned int, flags)
+{
+	struct uintr_uitt_ctx *uitt_ctx;
+	struct fd f;
+	int ret;
+
+	if (!cpu_feature_enabled(X86_FEATURE_UINTR))
+		return -ENOSYS;
+
+	if (flags)
+		return -EINVAL;
+
+	uitt_ctx = current->mm->context.uitt_ctx;
+	if (!uitt_ctx) {
+		pr_debug("send: unregister sender for task=%d, something might be wrong here\n",
+			 current->pid);
+		ret = -EINVAL;
+		goto out_fdput;
+	}
+
+	ret = do_uintr_unregister_sender(uitt_ctx, uipi_index);
+
+	pr_debug("send: unregister sender uipi_index %d for task=%d ret %d\n",
+		 uipi_index, current->pid, ret);
+
+out_fdput:
+	fdput(f);
+	return ret;
+}
+
+/*
+ * sys_uintr_register_self - Register self as UINTR sender (without creating an FD)
+ */
+SYSCALL_DEFINE2(uintr_register_self, u64, vector, unsigned int, flags)
+{
+	int ret;
+
+	if (!cpu_feature_enabled(X86_FEATURE_UINTR))
+		return -ENOSYS;
+
+	if (flags)
+		return -EINVAL;
+
+	/* Pass NULL since no FD is being created */
+	ret = do_uintr_register_vector(vector, NULL);
+	if (ret)
+		return ret;
+
+	ret = do_uintr_register_sender(vector, current->thread.upid_ctx);
+	if (ret < 0)
+		do_uintr_unregister_vector(vector, current->thread.upid_ctx);
+
+	return ret;
+}
+
 static int do_uintr_unregister_handler(void)
 {
 	struct task_struct *t = current;
@@ -529,7 +1115,7 @@ void uintr_free(struct task_struct *t)
 		return;
 
 	upid_ctx = t->thread.upid_ctx;
-	if (is_uintr_receiver(t)) {
+	if (is_uintr_task(t)) {
 		xstate = start_update_xsave_msrs(XFEATURE_UINTR);
 
 		xsave_wrmsrl(xstate, MSR_IA32_UINTR_MISC, 0);
@@ -546,10 +1132,12 @@ void uintr_free(struct task_struct *t)
 			 * generated based on this UPID.
 			 */
 			set_bit(UINTR_UPID_STATUS_SN, (unsigned long *)&upid_ctx->upid->nc.status);
+			upid_ctx->receiver_active = false;
 			put_upid_ref(upid_ctx);
 		}
 
 		t->thread.upid_activated = false;
+		t->thread.uitt_activated = false;
 
 		end_update_xsave_msrs();
 	}
@@ -565,4 +1153,13 @@ void uintr_free(struct task_struct *t)
 
 	//if (WARN_ON_ONCE(t != current))
 	//	return;
+#if 0
+	/* TODO: Fix exit flow */
+	if (is_uintr_sender(t)) {
+		/* TODO: Fix UITT dereferencing */
+		//put_uitt_ref(t->thread.ui_send->uitt_ctx);
+		//kfree(t->thread.ui_send);
+		t->thread.uitt_activated = false;
+	}
+#endif
 }
-- 
2.39.0

