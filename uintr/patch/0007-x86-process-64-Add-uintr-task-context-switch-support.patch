From a4bbf8e1047a73ad0cfd4c00c00edadcedabaad2 Mon Sep 17 00:00:00 2001
From: Sohil Mehta <sohil.mehta@intel.com>
Date: Wed, 7 Sep 2022 12:22:31 -0700
Subject: [PATCH 07/18] x86/process/64: Add uintr task context switch support

User interrupt state is saved and restored using xstate supervisor
feature support. This includes the MSR state and the User Interrupt Flag
(UIF) value.

During context switch update the UPID for a uintr task to reflect the
current state of the task; namely whether the task should receive
interrupt notifications and which cpu the task is currently running on.

XSAVES clears the notification vector (UINV) in the MISC MSR to prevent
interrupts from being recognized in the UIRR MSR while the task is being
context switched. The UINV is restored back when the kernel does an
XRSTORS.

However, this conflicts with the kernel's lazy restore optimization
which skips an XRSTORS if the kernel is scheduling the same user task
back and the underlying MSR state hasn't been modified. Special handling
is needed for a uintr task in the context switch path to keep using this
optimization.

Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
Signed-off-by: Sohil Mehta <sohil.mehta@intel.com>
---
 arch/x86/include/asm/entry-common.h |  4 ++
 arch/x86/include/asm/uintr.h        |  9 ++++
 arch/x86/kernel/fpu/core.c          |  8 +++
 arch/x86/kernel/process_64.c        |  4 ++
 arch/x86/kernel/uintr.c             | 75 +++++++++++++++++++++++++++++
 5 files changed, 100 insertions(+)

diff --git a/arch/x86/include/asm/entry-common.h b/arch/x86/include/asm/entry-common.h
index 674ed46d3ced..cb42f592a8f0 100644
--- a/arch/x86/include/asm/entry-common.h
+++ b/arch/x86/include/asm/entry-common.h
@@ -8,6 +8,7 @@
 #include <asm/nospec-branch.h>
 #include <asm/io_bitmap.h>
 #include <asm/fpu/api.h>
+#include <asm/uintr.h>
 
 /* Check that the stack and regs on entry from user mode are sane. */
 static __always_inline void arch_enter_from_user_mode(struct pt_regs *regs)
@@ -57,6 +58,9 @@ static inline void arch_exit_to_user_mode_prepare(struct pt_regs *regs,
 	if (unlikely(ti_work & _TIF_NEED_FPU_LOAD))
 		switch_fpu_return();
 
+	if (cpu_feature_enabled(X86_FEATURE_UINTR))
+		switch_uintr_return();
+
 #ifdef CONFIG_COMPAT
 	/*
 	 * Compat syscalls set TS_COMPAT.  Make sure we clear it before
diff --git a/arch/x86/include/asm/uintr.h b/arch/x86/include/asm/uintr.h
index ba64d825be9e..2c8156c72cc0 100644
--- a/arch/x86/include/asm/uintr.h
+++ b/arch/x86/include/asm/uintr.h
@@ -30,6 +30,15 @@ struct uintr_upid_ctx {
 	refcount_t refs;
 };
 
+/* TODO: Inline the context switch related functions */
+void switch_uintr_prepare(struct task_struct *prev);
+void switch_uintr_return(void);
+
+#else /* !CONFIG_X86_USER_INTERRUPTS */
+
+static inline void switch_uintr_prepare(struct task_struct *prev) {}
+static inline void switch_uintr_return(void) {}
+
 #endif /* CONFIG_X86_USER_INTERRUPTS */
 
 #endif /* _ASM_X86_UINTR_H */
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index 3b28c5b25e12..03801368d28f 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -111,6 +111,14 @@ static void update_avx_timestamp(struct fpu *fpu)
  * over the place.
  *
  * FXSAVE and all XSAVE variants preserve the FPU register state.
+ *
+ * When XSAVES is called with XFEATURE_UINTR enabled it
+ * saves the FPU state and clears the interrupt notification
+ * vector byte of the MISC_MSR [bits 39:32]. This is required
+ * to stop detecting additional User Interrupts after we
+ * have saved the FPU state. Before going back to userspace
+ * we would correct this and only program the byte that was
+ * cleared.
  */
 void save_fpregs_to_fpstate(struct fpu *fpu)
 {
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index 1962008fe743..75053dd8ffde 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -53,6 +53,7 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/vdso.h>
 #include <asm/resctrl.h>
+#include <asm/uintr.h>
 #include <asm/unistd.h>
 #include <asm/fsgsbase.h>
 #ifdef CONFIG_IA32_EMULATION
@@ -564,6 +565,9 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
 		     this_cpu_read(hardirq_stack_inuse));
 
+	if (cpu_feature_enabled(X86_FEATURE_UINTR))
+		switch_uintr_prepare(prev_p);
+
 	if (!test_thread_flag(TIF_NEED_FPU_LOAD))
 		switch_fpu_prepare(prev_fpu, cpu);
 
diff --git a/arch/x86/kernel/uintr.c b/arch/x86/kernel/uintr.c
index faff3dd361a5..8455b7a757ea 100644
--- a/arch/x86/kernel/uintr.c
+++ b/arch/x86/kernel/uintr.c
@@ -278,3 +278,78 @@ SYSCALL_DEFINE1(uintr_unregister_handler, unsigned int, flags)
 
 	return ret;
 }
+
+/* Suppress notifications since this task is being context switched out */
+void switch_uintr_prepare(struct task_struct *prev)
+{
+	struct uintr_upid_ctx *upid_ctx;
+
+	if (!is_uintr_receiver(prev))
+		return;
+
+	/* Check if UIF should be considered here. Do we want to wait for interrupts if UIF is 0? */
+	upid_ctx = prev->thread.upid_ctx;
+
+	set_bit(UINTR_UPID_STATUS_SN, (unsigned long *)&upid_ctx->upid->nc.status);
+}
+
+/*
+ * Do this right before we are going back to userspace after the FPU has been
+ * reloaded i.e. TIF_NEED_FPU_LOAD is clear.
+ * Called from arch_exit_to_user_mode_prepare() with interrupts disabled.
+ */
+void switch_uintr_return(void)
+{
+	struct uintr_upid *upid;
+	u64 misc_msr;
+
+	if (!is_uintr_receiver(current))
+		return;
+
+	/*
+	 * The XSAVES instruction clears the UINTR notification vector(UINV) in
+	 * the UINT_MISC MSR when user context gets saved. Before going back to
+	 * userspace we need to restore the notification vector. XRSTORS would
+	 * automatically restore the notification but we can't be sure that
+	 * XRSTORS will always be called when going back to userspace. Also if
+	 * XSAVES gets called twice the UINV stored in the Xstate buffer will
+	 * be overwritten. Threfore, before going back to userspace we always
+	 * check if the UINV is set and reprogram if needed.
+	 *
+	 * Alternatively, we could combine this with switch_fpu_return() and
+	 * program the MSR whenever we are skipping the XRSTORS. We need
+	 * special precaution to make sure the UINV value in the XSTATE buffer
+	 * doesn't get overwritten by calling XSAVES twice.
+	 */
+	WARN_ON_ONCE(test_thread_flag(TIF_NEED_FPU_LOAD));
+
+	/* Modify only the relevant bits of the MISC MSR */
+	rdmsrl(MSR_IA32_UINTR_MISC, misc_msr);
+	if (!(misc_msr & GENMASK_ULL(39, 32))) {
+		misc_msr |= (u64)UINTR_NOTIFICATION_VECTOR << 32;
+		wrmsrl(MSR_IA32_UINTR_MISC, misc_msr);
+	}
+
+	/*
+	 * It is necessary to clear the SN bit after we set UINV and NDST to
+	 * avoid incorrect interrupt routing.
+	 */
+	upid = current->thread.upid_ctx->upid;
+	upid->nc.ndst = cpu_to_ndst(smp_processor_id());
+	clear_bit(UINTR_UPID_STATUS_SN, (unsigned long *)&upid->nc.status);
+
+	/*
+	 * Interrupts might have accumulated in the UPID while the thread was
+	 * preempted. In this case invoke the hardware detection sequence
+	 * manually by sending a self IPI with UINV.  Since UINV is set and SN
+	 * is cleared, any new UINTR notifications due to the self IPI or
+	 * otherwise would result in the hardware updating the UIRR directly.
+	 * No real interrupt would be generated as a result of this.
+	 *
+	 * The alternative is to atomically read and clear the UPID and program
+	 * the UIRR. In that case the kernel would need to carefully manage the
+	 * race with the hardware if the UPID gets updated after the read.
+	 */
+	if (READ_ONCE(upid->puir))
+		apic->send_IPI_self(UINTR_NOTIFICATION_VECTOR);
+}
-- 
2.39.0

